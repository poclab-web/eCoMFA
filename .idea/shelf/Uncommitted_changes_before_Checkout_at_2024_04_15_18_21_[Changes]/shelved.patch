Index: CoMFA_model_lib/run_CoMFA.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import glob\nimport json\nimport multiprocessing\nimport os\nimport re\nimport sys\nimport time\nimport warnings\n\nimport cclib\nimport numpy as np\nimport pandas as pd\nimport scipy.linalg\nfrom rdkit.Chem import PandasTools\nfrom sklearn import linear_model\nfrom sklearn.cross_decomposition import PLSRegression\nfrom sklearn.metrics import r2_score, mean_squared_error\nfrom sklearn.model_selection import KFold\n\nimport calculate_conformation\n\nwarnings.simplefilter('ignore')\n\n\ndef Gaussian_penalized(df, dfp, gaussian_penalize, save_name,n_splits):\n    df_coord = pd.read_csv(gaussian_penalize + \"/coordinates_yz.csv\").sort_values(['x', 'y', \"z\"],\n                                                                                  ascending=[True, True, True])\n    features = [\"Dt\"]\n    features_all = np.array(df[features].values.tolist()).reshape(len(df), -1, len(features)).transpose(2, 0, 1)\n    # print(features_all.shape)\n    std = np.std(features_all, axis=(1, 2)).reshape(features_all.shape[0], 1, 1)\n    # print(std)\n    X = np.concatenate(features_all / std, axis=1)\n    # print(sys.getsizeof(X))\n    # XTX = (X.T @ X).astype(\"float32\")\n    # print(np.tril(XTX))\n\n    XTY = (X.T @ df[\"ΔΔG.expt.\"].values).astype(\"float32\")\n    # print(sys.getsizeof(XTX),sys.getsizeof(np.tril(XTX)))\n    # for n in range(1, 11):\n    gaussian_penalize=gaussian_penalize.replace(os.sep,'/')\n    print(sys.getsizeof(df_coord),sys.getsizeof(XTY))\n\n\n    for ptpname in sorted(glob.glob(gaussian_penalize + \"/ptp*.npy\")):\n        ptpname=ptpname.replace(os.sep,'/')\n        sigma = re.findall(gaussian_penalize + \"/ptp(.*).npy\", ptpname)\n        n = sigma[0]\n        start = time.time()\n        # ptp = np.load(ptpname)\n        print(\"before\", time.time() - start)\n        gaussians = []\n        predicts = []\n        for L in dfp[\"lambda\"]:\n            start = time.time()\n            gaussian_coef = scipy.linalg.solve(((X.T @ X).astype(\"float32\") + L * len(df) * 2 * np.load(ptpname)).astype(\"float32\"), XTY, assume_a=\"pos\").T\n            # print(\"before\", time.time() - start, gaussian_coef)\n            # x = np.sum(gaussian_coef * features, axis=1)\n            # a = np.dot(x, df[\"ΔΔG.expt.\"].values) / (x ** 2).sum()\n            # print(a)\n\n            gaussians.append(gaussian_coef)  # * a.tolist()\n            n_ = int(gaussian_coef.shape[0] / features_all.shape[0])\n            df_coord[\"Gaussian_Dt\"] = gaussian_coef[:n_]\n            df_coord.to_csv(save_name  + \"/molecular_field_csv\"+\"/molecular_filed{}{}.csv\".format(n, L))\n\n            kf = KFold(n_splits=int(n_splits), shuffle=False)\n            gaussian_predicts = []\n            start = time.time()\n\n            for (train_index, test_index) in kf.split(df):\n                # train_index=test_index\n                features_training = features_all[:, train_index]\n                std = np.std(features_training, axis=(1, 2)).reshape(features_all.shape[0], 1, 1)\n                # std = np.std(features_training, axis=(1)).reshape(features_all.shape[0], 1, -1)\n                X_ = np.concatenate(features_training / std, axis=1)\n                gaussian_coef_ = scipy.linalg.solve(\n                    (X_.T @ X_ + L * len(train_index) * 2 * np.load(ptpname)).astype(\"float32\"),\n                    (X_.T @ df.iloc[train_index][\"ΔΔG.expt.\"]).astype(\"float32\"),\n                    assume_a=\"pos\", check_finite=False, overwrite_a=True, overwrite_b=True).T\n                # x = np.sum(gaussian_coef_ * features_training, axis=1)\n                # a = np.dot(x, df.iloc[train_index][\"ΔΔG.expt.\"].values) / (x ** 2).sum()\n                features_test = features_all[:, test_index]\n                features_test = np.concatenate(features_test / std, axis=1)\n                predict = np.sum(gaussian_coef_ * features_test, axis=1).tolist()\n                gaussian_predicts.extend(predict)\n            # print(\"t=\", time.time() - start)\n\n            predicts.append([gaussian_predicts])\n        gaussians = np.array(gaussians)\n        gaussians = gaussians.reshape(gaussians.shape[0], 1, -1)\n        dfp[\"Gaussian_regression_predict{}\".format(n)] = np.sum(gaussians * X.reshape(1, X.shape[0], -1),\n                                                                axis=2).tolist()\n        dfp[[\"Gaussian_regression_r2{}\".format(n)]] = dfp[\n            [\"Gaussian_regression_predict{}\".format(n)]].applymap(\n            lambda predict: r2_score(df[\"ΔΔG.expt.\"], predict))\n        dfp[[\"Gaussian_regression_RMSE{}\".format(n)]] = \\\n            dfp[[\n                \"Gaussian_regression_predict{}\".format(n)]].applymap(\n                lambda predict: np.sqrt(mean_squared_error(df[\"ΔΔG.expt.\"], predict)))\n\n        dfp[[\"Gaussian_test_predict{}\".format(n)]] = predicts\n        dfp[[\"Gaussian_test_r2{}\".format(n)]] = dfp[\n            [\"Gaussian_test_predict{}\".format(n)]].applymap(\n            lambda predict: r2_score(df[\"ΔΔG.expt.\"], predict))\n        dfp[[\"Gaussian_test_r{}\".format(n)]] = dfp[\n            [\"Gaussian_test_predict{}\".format(n)]].applymap(\n            lambda predict: np.corrcoef(df[\"ΔΔG.expt.\"], predict)[1, 0])\n        dfp[[\"Gaussian_test_RMSE{}\".format(n)]] = dfp[\n            [\"Gaussian_test_predict{}\".format(n)]].applymap(\n            lambda predict: np.sqrt(mean_squared_error(df[\"ΔΔG.expt.\"], predict)))\n        df[\"Gaussian_predict{}\".format(n)] = \\\n            dfp[dfp[\"Gaussian_test_r{}\".format(n)] == dfp[\"Gaussian_test_r{}\".format(n)].max()].iloc[0][\n                \"Gaussian_test_predict{}\".format(n)]\n        df[\"Gaussian_error{}\".format(n)] = df[\"Gaussian_predict{}\".format(n)] - df[\"ΔΔG.expt.\"]\n        print(dfp[[\"Gaussian_test_RMSE{}\".format(n)]].min())\n\n    df = df.sort_values(by='Gaussian_error{}'.format(n), key=abs, ascending=[False])\n    PandasTools.AddMoleculeColumnToFrame(df, \"smiles\")\n    PandasTools.SaveXlsxFromFrame(df, save_name  + \"/σ_result.xlsx\", size=(100, 100))\n    dfp.to_csv(save_name  + \"/σ_result.csv\")\n    print(save_name)\n\n\ndef regression_comparison(df, dfp, gaussian_penalize, save_name, n,n_splits):\n    df_coord = pd.read_csv(gaussian_penalize + \"/coordinates_yz.csv\").sort_values(['x', 'y', \"z\"],\n                                                                                  ascending=[True, True, True])\n    # ptp = np.load(gaussian_penalize + \"/ptp{}.npy\".format(str(n)))\n    ptp_name=gaussian_penalize + \"/ptp{}.npy\".format(str(n))\n    features = [\"Dt\"]\n    features_all = np.array(df[features].values.tolist()).reshape(len(df), -1, len(features)).transpose(2, 0, 1)\n    std = np.std(features_all, axis=(1, 2)).reshape(features_all.shape[0], 1, 1)\n    features = np.concatenate(features_all / std, axis=1)\n\n    models = []\n    gaussians = []\n    predicts = []  # [[] for _ in range(4)]\n    for L, n_num in zip(dfp[\"lambda\"], dfp[\"n_components\"]):\n        ridge = linear_model.Ridge(alpha=L * len(df) * 2, fit_intercept=False).fit(features, df[\"ΔΔG.expt.\"])\n        # print(time.time()-start)\n        lasso = linear_model.Lasso(alpha=L, fit_intercept=False).fit(features, df[\"ΔΔG.expt.\"])\n        # print(time.time()-start)\n        # features_norm = features / np.std(features, axis=0)\n        # print(time.time()-start)\n        # pls = PLSRegression(n_components=n_num).fit(features_norm, df[\"ΔΔG.expt.\"])\n        pls = PLSRegression(n_components=n_num).fit(features, df[\"ΔΔG.expt.\"])\n        # print(time.time()-start)\n        models.append([ridge, lasso, pls])\n        # start = time.time()\n        X = features\n        Y = df[\"ΔΔG.expt.\"].values\n        gaussian_coef = scipy.linalg.solve(X.T @ X + L * len(df) * 2 * np.load(ptp_name), X.T @ Y, assume_a=\"pos\").T\n        # print(\"after_\", gaussian_coef, time.time() - start)\n\n        gaussians.append(gaussian_coef.tolist())\n        n = int(gaussian_coef.shape[0] / features_all.shape[0])\n        df_coord[\"Gaussian_Dt\"] = gaussian_coef[:n] * std[0].reshape([1])\n        # df_coord[\"Gaussian_ESP\"]=gaussian_coef[n:]\n        df_coord[\"Ridge_Dt\"] = ridge.coef_[:n] * std[0].reshape([1])\n        # df_coord[\"Ridge_ESP\"]=ridge.coef_[n:]\n        df_coord[\"Lasso_Dt\"] = lasso.coef_[:n] * std[0].reshape([1])\n        # df_coord[\"Lasso_ESP\"]=lasso.coef_[n:]\n        df_coord[\"PLS_Dt\"] = pls.coef_[0][:n] * std[0].reshape([1])\n        # df_coord[\"PLS_Dt\"] = pls.coef_[0][:n] * np.std(features, axis=0) * std[0].reshape([1])\n\n        kf = KFold(n_splits=int(n_splits), shuffle=False)\n        gaussian_predicts = []\n        ridge_predicts = []\n        lasso_predicts = []\n        pls_predicts = []\n\n        for i, (train_index, test_index) in enumerate(kf.split(df)):\n            features_training = features_all[:, train_index]\n            std_ = np.std(features_training, axis=(1, 2)).reshape(features_all.shape[0], 1, 1)\n            features_training = features_training / std_\n            features_training = np.concatenate(features_training, axis=1)\n            # start = time.time()\n            X = features_training\n            Y = df.iloc[train_index][\"ΔΔG.expt.\"].values\n            gaussian_coef_ = scipy.linalg.solve(X.T @ X + L * len(train_index) * 2 * np.load(ptp_name), X.T @ Y,\n                                                assume_a=\"pos\").T\n            # print(\"after__\",gaussian_coef_,time.time()-start)\n            # print(time.time()-start)\n            ridge = linear_model.Ridge(alpha=L * len(train_index) * 2, fit_intercept=False).fit(\n                features_training,\n                df.iloc[train_index][\n                    \"ΔΔG.expt.\"])\n            lasso = linear_model.Lasso(alpha=L, fit_intercept=False).fit(features_training,\n                                                                         df.iloc[train_index][\"ΔΔG.expt.\"])\n            # features_training_norm = features_training / np.std(features_training, axis=0)\n            # pls = PLSRegression(n_components=n_num).fit(features_training_norm,\n            #                                             df.iloc[train_index][\"ΔΔG.expt.\"])\n            pls = PLSRegression(n_components=n_num).fit(features_training,\n                                                        df.iloc[train_index][\"ΔΔG.expt.\"])\n            features_test = features_all[:, test_index]  # np.array(features_all)[test_index].transpose(2, 0, 1)\n            features_test = np.concatenate(features_test / std_, axis=1)\n            predict = np.sum(gaussian_coef_ * features_test, axis=1).tolist()  # model.predict(features_test)\n            gaussian_predicts.extend(predict)\n            # predicts[0].extend(predict)\n            ridge_predict = ridge.predict(features_test)\n            # predicts[1].extend(ridge_predict)\n            ridge_predicts.extend(ridge_predict)\n            lasso_predict = lasso.predict(features_test)\n            # predicts[2].extend(lasso_predict)\n            lasso_predicts.extend(lasso_predict)\n            # features_test_norm = features_test / np.std(features_training, axis=0)\n            # pls_predict = pls.predict(features_test_norm)\n            pls_predict = pls.predict(features_test)\n            # predicts[3].extend([_[0] for _ in pls_predict])\n            pls_predicts.extend([_[0] for _ in pls_predict])\n            n = int(gaussian_coef_.shape[0] / features_all.shape[0])\n\n            df_coord[\"Gaussian_Dt{}\".format(i)] = gaussian_coef_[:n] * std_[0].reshape([1])\n            df_coord[\"Ridge_Dt{}\".format(i)] = ridge.coef_[:n] * std_[0].reshape([1])\n            df_coord[\"Lasso_Dt{}\".format(i)] = lasso.coef_[:n] * std_[0].reshape([1])\n            # df_coord[\"PLS_Dt{}\".format(i)] = pls.coef_[0][:n] * np.std(features, axis=0) * std_[0].reshape([1])\n            df_coord[\"PLS_Dt{}\".format(i)] = pls.coef_[0][:n] * std_[0].reshape([1])\n\n        predicts.append([gaussian_predicts, ridge_predicts, lasso_predicts, pls_predicts])\n        df_coord.to_csv(save_name + \"/molecular_field_csv\" + \"/molecular_field{}.csv\".format(L))\n    print(np.array(predicts).shape)\n    dfp[[\"Ridge_model\", \"Lasso_model\", \"PLS_model\"]] = models\n    dfp[[\"Ridge_regression_predict\", \"Lasso_regression_predict\", \"PLS_regression_predict\"]] \\\n        = dfp[[\"Ridge_model\", \"Lasso_model\", \"PLS_model\"]].applymap(lambda model: model.predict(features))\n    gaussians = np.array(gaussians)\n    gaussians = gaussians.reshape(gaussians.shape[0], 1, -1)\n    features = features.reshape(1, features.shape[0], -1)\n    dfp[\"Gaussian_regression_predict\"] = np.sum(gaussians * features, axis=2).tolist()\n    dfp[[\"Gaussian_regression_r2\", \"Ridge_regression_r2\", \"Lasso_regression_r2\", \"PLS_regression_r2\"]] = dfp[\n        [\"Gaussian_regression_predict\", \"Ridge_regression_predict\", \"Lasso_regression_predict\",\n         \"PLS_regression_predict\"]].applymap(\n        lambda predict: r2_score(df[\"ΔΔG.expt.\"], predict))\n    dfp[[\"Gaussian_regression_RMSE\", \"Ridge_regression_RMSE\", \"Lasso_regression_RMSE\", \"PLS_regression_RMSE\"]] = dfp[[\n        \"Gaussian_regression_predict\", \"Ridge_regression_predict\", \"Lasso_regression_predict\",\n        \"PLS_regression_predict\"]].applymap(\n        lambda predict: np.sqrt(mean_squared_error(df[\"ΔΔG.expt.\"], predict)))\n\n    dfp[[\"Gaussian_test_predict\", \"ridge_test_predict\", \"lasso_test_predict\", \"pls_test_predict\"]] = predicts\n    dfp[[\"Gaussian_test_r2\", \"ridge_test_r2\", \"lasso_test_r2\", \"pls_test_r2\"]] = dfp[\n        [\"Gaussian_test_predict\", \"ridge_test_predict\", \"lasso_test_predict\", \"pls_test_predict\"]].applymap(\n        lambda predict: r2_score(df[\"ΔΔG.expt.\"], predict))\n    dfp[[\"Gaussian_test_r\", \"ridge_test_r\", \"lasso_test_r\", \"pls_test_r\"]] = dfp[\n        [\"Gaussian_test_predict\", \"ridge_test_predict\", \"lasso_test_predict\", \"pls_test_predict\"]].applymap(\n        lambda predict: np.corrcoef(df[\"ΔΔG.expt.\"], predict)[1, 0])\n    dfp[[\"Gaussian_test_RMSE\", \"ridge_test_RMSE\", \"lasso_test_RMSE\", \"pls_test_RMSE\"]] = dfp[\n        [\"Gaussian_test_predict\", \"ridge_test_predict\", \"lasso_test_predict\", \"pls_test_predict\"]].applymap(\n        lambda predict: np.sqrt(mean_squared_error(df[\"ΔΔG.expt.\"], predict)))\n\n    features_R1 = np.array(df[\"DtR1\"].tolist()).reshape(len(df), -1, 1).transpose(2, 0, 1)\n    features_R1 = np.concatenate(features_R1 / std, axis=1)\n    dfp[\"Gaussian_regression_predictR1\"] = np.sum(gaussians * features_R1, axis=2).tolist()\n    dfp[[\"Ridge_regression_predictR1\", \"Lasso_regression_predictR1\", \"PLS_regression_predictR1\"]] \\\n        = dfp[[\"Ridge_model\", \"Lasso_model\", \"PLS_model\"]].applymap(lambda model: model.predict(features_R1))\n\n    features_R2 = np.array(df[\"DtR2\"].tolist()).reshape(len(df), -1, 1).transpose(2, 0, 1)\n    features_R2 = np.concatenate(features_R2 / std, axis=1)\n    dfp[\"Gaussian_regression_predictR2\"] = np.sum(gaussians * features_R2, axis=2).tolist()\n    dfp[[\"Ridge_regression_predictR2\", \"Lasso_regression_predictR2\", \"PLS_regression_predictR2\"]] \\\n        = dfp[[\"Ridge_model\", \"Lasso_model\", \"PLS_model\"]].applymap(lambda model: model.predict(features_R2))\n\n    df[[\"Gaussian_test\", \"Gaussian_regression\", \"Gaussian_R1\", \"Gaussian_R2\"]] = \\\n        dfp.loc[dfp[\"Gaussian_test_r\"].idxmax()][\n            [\"Gaussian_test_predict\", \"Gaussian_regression_predict\", \"Gaussian_regression_predictR1\",\n             \"Gaussian_regression_predictR2\"]]\n    df[\"Ridge_test\"] = dfp.loc[dfp[\"ridge_test_r\"].idxmax()][\"ridge_test_predict\"]\n    df[\"Lasso_test\"] = dfp.loc[dfp[\"lasso_test_r\"].idxmax()][\"lasso_test_predict\"]\n    df[\"PLS_test\"] = dfp.loc[dfp[\"pls_test_r\"].idxmax()][\"pls_test_predict\"]\n\n    df[\"Gaussian_error\"] = df[\"Gaussian_test\"] - df[\"ΔΔG.expt.\"]\n    # df[[\"Gaussian_error\",\"Ridge_error\",\"Lasso_error\"]] = df[[\"Gaussian_test\",\"Ridge_test\",\"Lasso_test\",]].applymap(lambda test:test - df[\"ΔΔG.expt.\"].values)\n    df = df.sort_values(by='Gaussian_error', key=abs, ascending=[False])\n    PandasTools.AddMoleculeColumnToFrame(df, \"smiles\")\n    print(dfp[[\"Gaussian_test_r2\", \"ridge_test_r2\", \"lasso_test_r2\", \"pls_test_r2\"]].max())\n    PandasTools.SaveXlsxFromFrame(df, save_name + \"/λ_result.xlsx\", size=(100, 100))\n    dfp.to_csv(save_name + \"/λ_result.csv\", index=False)\n    print(save_name)\n\n\n# def energy_to_Boltzmann_distribution(mol, RT=1.99e-3 * 273):\n#     energies = np.array([float(conf.GetProp(\"energy\")) for conf in mol.GetConformers()])\n#     energies = energies - np.min(energies)\n#     rates = np.exp(-energies / RT)\n#     rates = rates / sum(rates)\n#     for conf, rate in zip(mol.GetConformers(), rates):\n#         conf.SetProp(\"Boltzmann_distribution\", str(rate))\n\ndef energy_to_Boltzmann_distribution(mol, RT=1.99e-3 * 273):\n    energies = []\n    for conf in mol.GetConformers():\n        # print(conf.GetId())\n        line = json.loads(conf.GetProp(\"freq\"))\n        energies.append(float(line[0] - line[1] * RT / 1.99e-3))\n    energies = np.array(energies)\n    energies = energies - np.min(energies)\n    rates = np.exp(-energies / RT)\n    rates = rates / np.sum(rates)\n    for conf, rate in zip(mol.GetConformers(), rates):\n        conf.SetProp(\"Boltzmann_distribution\", str(rate))\n\n\ndef is_normal_frequencies(filename):\n    try:\n        with open(filename, 'r') as f:\n            lines = f.readlines()\n            frequencies_lines = [line for line in lines if 'Frequencies' in line]\n            for l in frequencies_lines:\n                splited = l.split()\n                values = splited[2:]\n                values = [float(v) for v in values]\n                for v in values:\n                    if v < 0:\n                        f.close()\n                        print(filename, \" is bad conformer\")\n                        return False\n            f.close()\n        return True\n    except:\n        print(filename,\" is bad conformer\")\n        return False\n\n\n# def select_σ_n(file_name):\n#     Gaussian = [[] for i in range(10)]\n#     for _ in range(5):\n#         save_path = param[\"out_dir_name\"] + \"/\" + file_name + \"/comparison\" + str(_)\n#         print(save_path)\n#         dfp_n = pd.read_csv(save_path + \"/n_comparison.csv\")\n#         for j in range(1, 11):\n#             Gaussian[j - 1].append(dfp_n[\"Gaussian_test_r2{}\".format(j)].values.tolist())\n#             # ax.plot(dfp[\"lambda\"], dfp[\"lasso_test_r2\"], color=\"red\", linewidth=1, alpha=0.05)\n#             # ax.plot(dfp[\"lambda\"], dfp[\"ridge_test_r2\"], color=\"green\", linewidth=1, alpha=0.05)\n#             # ax2.plot(range(1, len(dfp[\"lambda\"]) + 1), dfp[\"pls_test_r2\"], color=\"orange\", linewidth=1, alpha=0.05)\n#     gave = [[] for i in range(10)]\n#     for j in range(1, 11):\n#         gave[j - 1].append(np.average(Gaussian[j - 1]))\n#     # print(Gaussian)\n#     print(\"gave\")\n#     print(gave)\n#\n#     print(np.argmax(gave))\n#     q = np.argmax(gave)\n#     n = q + 1\n#     print(n)\n#     return n\n# print(Gaussian[j-1])\n# ax.plot(dfp[\"lambda\"], np.average(Gaussian[j - 1], axis=0), \"o\",\n#         label=\"n = \" + str(j) + \"\\n{:.3f}\".format(),\n#         color=cm.hsv(j / 10), alpha=1)\n\n\ndef GP(input):\n    df_, dfp, grid_coordinates, save_path,n_splits = input\n    Gaussian_penalized(df_, dfp, grid_coordinates, save_path,n_splits)\n\n\ndef RC(input):\n    df_, dfp, grid_coordinates, save_path, n,n_splits = input\n    regression_comparison(df_, dfp, grid_coordinates, save_path, n,n_splits)\ndef run(input):\n    df_, dfp, grid_coordinates, save_path, n, n_splits,flag = input\n    if flag:\n        Gaussian_penalized(df_, dfp, grid_coordinates, save_path, n_splits)\n    else:\n        regression_comparison(df_, dfp, grid_coordinates, save_path, n,n_splits)\n\nif __name__ == '__main__':\n    #time.sleep(60*60*24*2.5)\n    inputs=[]\n    inputs_=[]\n    for param_name in sorted(glob.glob(\"../parameter/cube_to_grid/cube_to_grid0.*.txt\"),reverse=False):\n        with open(param_name, \"r\") as f:\n            param = json.loads(f.read())\n        print(param)\n        start = time.perf_counter()  # 計測開始\n        for file in glob.glob(\"../arranged_dataset/*.xlsx\"):\n        # for file in glob.glob(\"../arranged_dataset/*.xlsx\"):\n\n            df = pd.read_excel(file).dropna(subset=['smiles']).reset_index(drop=True)  # [:50]\n            print(len(df))\n            file_name = os.path.splitext(os.path.basename(file))[0]\n            features_dir_name = param[\"grid_coordinates\"] + file_name\n            print(features_dir_name)\n            df[\"mol\"] = df[\"smiles\"].apply(calculate_conformation.get_mol)\n            # df = df[[os.path.isdir(features_dir_name + \"/\" + mol.GetProp(\"InchyKey\")) for mol in df[\"mol\"]]]\n            print(len(df))\n            df = df[\n                [len(glob.glob(\"{}/{}/*\".format(param[\"grid_coordinates\"], mol.GetProp(\"InchyKey\"))))>0 for mol in\n                 df[\"mol\"]]]\n            print(len(df))\n\n            df = df[[os.path.isdir(\"{}/{}\".format(param[\"freq_dir\"], mol.GetProp(\"InchyKey\"))) for mol in\n                     df[\"mol\"]]]\n            # df = df[\n            #     [os.path.isfile(\"{}/{}/data0.pkl\".format(param[\"grid_coordinates\"], mol.GetProp(\"InchyKey\"))) for mol in\n            #      df[\"mol\"]]]\n            freq = []\n            for mol in df[\"mol\"]:\n                freq_ = any([is_normal_frequencies(path) for path in\n                             sorted(\n                                 glob.glob(\n                                     param[\"freq_dir\"] + \"/\" + mol.GetProp(\"InchyKey\") + \"/gaussianinput?.log\"))])\n\n                # print(mol.GetProp(\"InchyKey\"), freq_)\n                freq.append(freq_)\n            df=df[freq]\n            df[\"mol\"].apply(\n                lambda mol: calculate_conformation.read_xyz(mol,\n                                                            param[\"cube_dir_name\"] + \"/\" + mol.GetProp(\"InchyKey\")))\n            grid = []\n            for mol in df[\"mol\"]:\n                freq_ = all([os.path.isfile(\n                    \"{}/{}/data{}.pkl\".format(param[\"grid_coordinates\"], mol.GetProp(\"InchyKey\"), conf.GetId())) for\n                    conf in mol.GetConformers()])\n\n                # print(mol.GetProp(\"InchyKey\"), freq_)\n                grid.append(freq_)\n            df = df[grid]\n            print(\"dflen\", len(df))\n\n            for mol in df[\"mol\"]:\n                dirs_name_freq = param[\"freq_dir\"] + \"/\" + mol.GetProp(\"InchyKey\") + \"/gaussianinput?.log\"\n                print(dirs_name_freq)\n                del_list=[]\n                for path, conf in zip(\n                        sorted(glob.glob(dirs_name_freq)),\n                        mol.GetConformers()):\n                    # print(path)\n                    if is_normal_frequencies(path):\n                        data = cclib.io.ccread(path)\n                        ent = data.enthalpy * 627.5095  # hartree\n                        entr = data.entropy * 627.5095  # hartree\n                        conf.SetProp(\"freq\", json.dumps([ent, entr]))\n                        # print(conf.GetProp(\"freq\"))\n                    else:\n                        del_list.append(conf.GetId())\n                for _ in del_list:\n                    mol.RemoveConformer(_)\n\n\n            print(\"dflen\", len(df))\n            dfp = pd.read_csv(param[\"grid_coordinates\"] + \"/penalty_param.csv\")\n            Dts = []\n            DtR1 = []\n            DtR2 = []\n            ESPs = []\n            ESPR1 = []\n            ESPR2 = []\n            for mol, RT in df[[\"mol\", \"RT\"]].values:\n                print(mol.GetProp(\"InchyKey\"))\n                energy_to_Boltzmann_distribution(mol, RT)\n                Dt = []\n                ESP = []\n                we = []\n                for conf in mol.GetConformers():\n                    data = pd.read_pickle(\n                        \"{}/{}/data{}.pkl\".format(param[\"grid_coordinates\"], mol.GetProp(\"InchyKey\"), conf.GetId()))\n                    Bd = float(conf.GetProp(\"Boltzmann_distribution\"))\n                    we.append(Bd)\n                    Dt.append(data[\"Dt\"].values.tolist())\n                    ESP.append(data[\"ESP\"].values.tolist())\n                Dt = np.array(Dt)\n                ESP = np.array(ESP)\n                w = np.exp(-Dt / np.sqrt(np.average(Dt ** 2, axis=0)).reshape(1, -1))\n                data[\"Dt\"] = np.nan_to_num(\n                    np.average(Dt, weights=np.array(we).reshape(-1, 1) * np.ones(shape=Dt.shape), axis=0))\n                w = np.exp(ESP / np.sqrt(np.average(ESP ** 2, axis=0)).reshape(1, -1))\n                data[\"ESP\"] = np.nan_to_num(\n                    np.average(ESP, weights=np.array(we).reshape(-1, 1) * np.ones(shape=ESP.shape), axis=0))\n                data_y = data[data[\"y\"] > 0].sort_values(['x', 'y', \"z\"], ascending=[True, True, True])\n                data_y[\"Dt\"] = data[data[\"y\"] > 0].sort_values(['x', 'y', \"z\"], ascending=[True, True, True])[\n                                   \"Dt\"].values + \\\n                               data[data[\"y\"] < 0].sort_values(['x', 'y', \"z\"], ascending=[True, False, True])[\n                                   \"Dt\"].values\n                data_y[\"ESP\"] = data[data[\"y\"] > 0].sort_values(['x', 'y', \"z\"], ascending=[True, True, True])[\n                                    \"ESP\"].values + \\\n                                data[data[\"y\"] < 0].sort_values(['x', 'y', \"z\"], ascending=[True, False, True])[\n                                    \"ESP\"].values\n                data_yz = data_y[data_y[\"z\"] > 0].sort_values(['x', 'y', \"z\"], ascending=[True, True, True])\n                data_yz[\"Dt\"] = data_y[data_y[\"z\"] > 0].sort_values(['x', 'y', \"z\"], ascending=[True, True, True])[\n                                    \"Dt\"].values - \\\n                                data_y[data_y[\"z\"] < 0].sort_values(['x', 'y', \"z\"], ascending=[True, True, False])[\n                                    \"Dt\"].values\n                data_yz[\"ESP\"] = data_y[data_y[\"z\"] > 0].sort_values(['x', 'y', \"z\"], ascending=[True, True, True])[\n                                     \"ESP\"].values - \\\n                                 data_y[data_y[\"z\"] < 0].sort_values(['x', 'y', \"z\"], ascending=[True, True, False])[\n                                     \"ESP\"].values\n                data_yz[\"DtR1\"] = \\\n                    data_y[data_y[\"z\"] > 0].sort_values(['x', 'y', \"z\"], ascending=[True, True, True])[\"Dt\"].values\n                data_yz[\"DtR2\"] = \\\n                    data_y[data_y[\"z\"] < 0].sort_values(['x', 'y', \"z\"], ascending=[True, True, False])[\"Dt\"].values\n                data_yz[\"ESPR1\"] = \\\n                    data_y[data_y[\"z\"] > 0].sort_values(['x', 'y', \"z\"], ascending=[True, True, True])[\"ESP\"].values\n                data_yz[\"ESPR2\"] = \\\n                    data_y[data_y[\"z\"] < 0].sort_values(['x', 'y', \"z\"], ascending=[True, True, False])[\"ESP\"].values\n                dfp_yz = data_yz.copy()\n\n                Dts.append(dfp_yz[\"Dt\"].values.tolist())\n                DtR1.append(dfp_yz[\"DtR1\"].values.tolist())\n                DtR2.append(dfp_yz[\"DtR2\"].values.tolist())\n                ESPs.append(dfp_yz[\"ESP\"].values.tolist())\n                ESPR1.append(dfp_yz[\"ESPR1\"].values.tolist())\n                ESPR2.append(dfp_yz[\"ESPR2\"].values.tolist())\n\n            df[\"Dt\"] = Dts\n            df[\"DtR1\"] = DtR1\n            df[\"DtR2\"] = DtR2\n            df[\"ESP\"] = ESPs\n            df[\"ESPR1\"] = ESPR1\n            df[\"ESPR2\"] = ESPR2\n\n            print(\"feature_calculated\")\n            # inputs = []\n            # for _ in range(10):\n            #     df_ = df.sample(frac=1, random_state=_)\n            #     save_path = param[\"out_dir_name\"] + \"/\" + file_name + \"/comparison\" + str(_)\n            #     os.makedirs(save_path, exist_ok=True)\n            #     input = df_, dfp, param[\"grid_coordinates\"], save_path,param[\"n_splits\"]\n            #     inputs.append(input)\n            # num_processes = multiprocessing.cpu_count()\n            # print(num_processes)\n            # p = multiprocessing.Pool(processes=20)\n            # p.map(GP, inputs)\n            n = param[\"sigma\"]\n            # inputs = []\n            # for _ in range(10):\n            #     df_ = df.sample(frac=1, random_state=_)\n            #     save_path = param[\"out_dir_name\"] + \"/\" + file_name + \"/\" + str(_)\n            #     os.makedirs(save_path, exist_ok=True)\n            #     input = df_, dfp, param[\"grid_coordinates\"], save_path, n,param[\"n_splits\"]\n            #     inputs.append(input)\n            # p.map(RC, inputs)\n            for _ in range(10):\n                df_ = df.sample(frac=1, random_state=_)\n                save_path = param[\"out_dir_name\"] + \"/\" + file_name + \"/\" + str(_)\n                os.makedirs(save_path, exist_ok=True)\n                input = df_, dfp, param[\"grid_coordinates\"], save_path, n, param[\"n_splits\"],True\n                inputs.append(input)\n                input = df_, dfp, param[\"grid_coordinates\"], save_path, n, param[\"n_splits\"], False\n                inputs_.append(input)\n            # p.map(run,inputs)\n        # end = time.perf_counter()  # 計測終了\n        # print('Finish{:.2f}'.format(end - start))\n    p = multiprocessing.Pool(processes=30)\n    p.map(run, inputs)\n    p = multiprocessing.Pool(processes=15)\n    p.map(run, inputs_)\n    end = time.perf_counter()  # 計測終了\n    print('Finish{:.2f}'.format(end - start))
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/CoMFA_model_lib/run_CoMFA.py b/CoMFA_model_lib/run_CoMFA.py
--- a/CoMFA_model_lib/run_CoMFA.py	
+++ b/CoMFA_model_lib/run_CoMFA.py	
@@ -40,9 +40,10 @@
     # for n in range(1, 11):
     gaussian_penalize=gaussian_penalize.replace(os.sep,'/')
     print(sys.getsizeof(df_coord),sys.getsizeof(XTY))
-
+    os.makedirs(save_name + "/molecular_field_csv", exist_ok=True)
 
     for ptpname in sorted(glob.glob(gaussian_penalize + "/ptp*.npy")):
+
         ptpname=ptpname.replace(os.sep,'/')
         sigma = re.findall(gaussian_penalize + "/ptp(.*).npy", ptpname)
         n = sigma[0]
@@ -62,6 +63,7 @@
             gaussians.append(gaussian_coef)  # * a.tolist()
             n_ = int(gaussian_coef.shape[0] / features_all.shape[0])
             df_coord["Gaussian_Dt"] = gaussian_coef[:n_]
+
             df_coord.to_csv(save_name  + "/molecular_field_csv"+"/molecular_filed{}{}.csv".format(n, L))
 
             kf = KFold(n_splits=int(n_splits), shuffle=False)
@@ -123,6 +125,7 @@
 
 
 def regression_comparison(df, dfp, gaussian_penalize, save_name, n,n_splits):
+    os.makedirs(save_name + "/molecular_field_csv",exist_ok=True)
     df_coord = pd.read_csv(gaussian_penalize + "/coordinates_yz.csv").sort_values(['x', 'y', "z"],
                                                                                   ascending=[True, True, True])
     # ptp = np.load(gaussian_penalize + "/ptp{}.npy".format(str(n)))
@@ -365,9 +368,10 @@
 
 if __name__ == '__main__':
     #time.sleep(60*60*24*2.5)
-    inputs=[]
-    inputs_=[]
-    for param_name in sorted(glob.glob("../parameter/cube_to_grid/cube_to_grid0.*.txt"),reverse=False):
+
+    for param_name in sorted(glob.glob("../parameter/cube_to_grid/cube_to_grid0.50*.txt"),reverse=False):
+        inputs = []
+        inputs_ = []
         with open(param_name, "r") as f:
             param = json.loads(f.read())
         print(param)
@@ -540,9 +544,9 @@
             # p.map(run,inputs)
         # end = time.perf_counter()  # 計測終了
         # print('Finish{:.2f}'.format(end - start))
-    p = multiprocessing.Pool(processes=30)
-    p.map(run, inputs)
-    p = multiprocessing.Pool(processes=15)
-    p.map(run, inputs_)
-    end = time.perf_counter()  # 計測終了
-    print('Finish{:.2f}'.format(end - start))
\ No newline at end of file
+        p = multiprocessing.Pool(processes=15)
+        p.map(run, inputs)
+        p = multiprocessing.Pool(processes=15)
+        p.map(run, inputs_)
+        end = time.perf_counter()  # 計測終了
+        print('Finish{:.2f}'.format(end - start))
\ No newline at end of file
